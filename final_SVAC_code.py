# -*- coding: utf-8 -*-
"""FINAL_Mayte_Linda_Sexual Violence in armed conflicts.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fb_JvDv1WU4rf7a1ppxqJZoO4KyziWDH
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import f1_score
from imblearn.over_sampling import RandomOverSampler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from google.colab import files

df = pd.read_excel('SVAC_3.2_complete.xlsx')
df.head(10)



# Analyze'state_prev', 'ai_prev', 'hrw_prev', 'child_prev'
print(df[['state_prev', 'ai_prev', 'hrw_prev', 'child_prev']].describe())
print(df[['state_prev', 'ai_prev', 'hrw_prev', 'child_prev']].head())

# Focus on the relevant columns for analysis: actor, state_prev, ai_prev, hrw_prev, child_prev
# Summarize and visualize the data based on these columns

# First: Extract relevant columns
relevant_columns = ['actor', 'state_prev', 'ai_prev', 'hrw_prev', 'child_prev']
df_relevant = df[relevant_columns]

# Now we will analyze the data for each type of documentation status
summary = {
    'Massive Violence (3)': (df_relevant == 3).sum(),
    'Numerous Violence (2)': (df_relevant == 2).sum(),
    'Isolated Violence (1)': (df_relevant == 1).sum(),
    'No Violence (0)': (df_relevant == 0).sum(),
    'No Report (-99)': (df_relevant == -99).sum(),
}

summary_df = pd.DataFrame(summary)

import matplotlib.pyplot as plt

# Create bar plots for each category of documentation status

categories = ['Massive Violence (3)', 'Numerous Violence (2)', 'Isolated Violence (1)', 'No Violence (0)', 'No Report (-99)']
colors = ['red', 'orange', 'yellow', 'green', 'gray']
sources = ['state_prev', 'ai_prev', 'hrw_prev', 'child_prev']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for i, source in enumerate(sources):
    axes[i].bar(categories, summary_df.loc[source], color=colors)
    axes[i].set_title(f'Sexual Violence Documentation: {source}')
    axes[i].set_ylabel('Frequency')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Count the unique number of actors in the dataset
unique_actors = df['actor'].nunique()
unique_actors

plt.figure(figsize=(12, 8))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Heatmap of Missing Values in the Entire Dataset')
plt.show()

# Create 'sexual_violence' column: 1 if any source reports violence (1, 2, or 3), otherwise 0
df['sexual_violence'] = df_relevant.apply(lambda x: 1 if any((x == 1) | (x == 2) | (x == 3)) else 0, axis=1)

print(df['sexual_violence'].value_counts())
df.head(10)

import seaborn as sns
plt.figure(figsize=(8, 6))
sns.countplot(x='sexual_violence', data=df)
plt.title('Distribution of Sexual Violence Observations')
plt.xlabel('Sexual Violence')
plt.ylabel('Count of Observations')
plt.show()

sexual_violence_counts = df['sexual_violence'].value_counts()
plt.figure(figsize=(8, 8))
plt.pie(sexual_violence_counts, labels=sexual_violence_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Sexual Violence Observations')
plt.show()

#How many cases do we have in which there is a documentation
# Filter the dataset to include only rows where documentation is available (no -99 in relevant columns)
df_filtered = df[(df_relevant != -99).all(axis=1)]

# Check the size of the filtered dataset
df_filtered.shape

# Filter the dataset to identify cases where sexual violence was documented (codes 1, 2, 3) and where no sexual violence was documented (code 0)

# Cases where sexual violence was documented
violence_documented = df_relevant[(df_relevant == 1) | (df_relevant == 2) | (df_relevant == 3)].any(axis=1)
actors_with_violence = df.loc[violence_documented, 'actor'].value_counts().head(10)

# Cases where no sexual violence was documented but reports exist
no_violence_documented = df_relevant[(df_relevant == 0)].all(axis=1)
actors_with_no_violence = df.loc[no_violence_documented, 'actor'].value_counts().head(10)

actors_with_violence, actors_with_no_violence

# Development of Conflicts and Sexual violence in Conflicts by Year

# Count the total number of conflicts per year
total_conflicts = df_filtered.groupby('year').size()

# Count the number of conflicts with any sexual violence (where sexual_violence is 1, 2, or 3)
conflicts_with_sv = df_filtered[df_filtered['sexual_violence'].isin([1, 2, 3])].groupby('year').size()

# Combine these into a DataFrame
data = pd.DataFrame({
    'Total Conflicts': total_conflicts,
    'Conflicts with Sexual Violence': conflicts_with_sv
}).fillna(0)  # Fill NaN values with 0

# Step 2: Create Stacked Bar Chart

plt.figure(figsize=(12, 8))

# Plot the total number of conflicts in yellow
plt.bar(data.index, data['Total Conflicts'], color='yellow', label='Total Conflicts')

# Plot the number of conflicts with sexual violence in red, stacked on top of the yellow bars
plt.bar(data.index, data['Conflicts with Sexual Violence'], color='red', label='Conflicts with Sexual Violence')

# Add labels and title
plt.title('Development of Conflicts and Sexual Violence in Conflicts by Year')
plt.xlabel('Year')
plt.ylabel('Number of Conflicts')
plt.legend(title='Conflict Data')
plt.grid(True)

plt.show()

# Visualize the top 10 actors with documented sexual violence and those with documentation but no violence

# Focus on the relevant columns for analysis: actor, state_prev, ai_prev, hrw_prev, child_prev
relevant_columns = ['actor', 'state_prev', 'ai_prev', 'hrw_prev', 'child_prev']
df_relevant = df[relevant_columns]

# Replace -99 with NaN to ensure missing data is handled appropriately
df_relevant = df_relevant.replace(-99, np.nan)

# Create a binary column 'sexual_violence' where 1 indicates any form of sexual violence (1, 2, or 3), and 0 indicates no violence (0)
df['sexual_violence'] = df_relevant.apply(lambda x: 1 if any((x == 1) | (x == 2) | (x == 3)) else 0, axis=1)

# Filter the dataset to identify cases where sexual violence was documented and where no sexual violence was documented
violence_documented = df['sexual_violence'] == 1
no_violence_documented = df['sexual_violence'] == 0

# Count the top 10 actors with documented sexual violence and those with no documented violence
actors_with_violence = df.loc[violence_documented, 'actor'].value_counts().head(10)
actors_with_no_violence = df.loc[no_violence_documented, 'actor'].value_counts().head(10)

# Visualize the top 10 actors with documented sexual violence and those with documentation but no violence
fig, axes = plt.subplots(1, 2, figsize=(18, 6))

# Bar plot for actors with documented sexual violence
axes[0].barh(actors_with_violence.index, actors_with_violence.values, color='red')
axes[0].set_title('Top 10 Actors with Documented Sexual Violence')
axes[0].set_xlabel('Number of Cases')
axes[0].invert_yaxis()

# Bar plot for actors with documentation but no sexual violence
axes[1].barh(actors_with_no_violence.index, actors_with_no_violence.values, color='green')
axes[1].set_title('Top 10 Actors with Documentation but No Sexual Violence')
axes[1].set_xlabel('Number of Cases')
axes[1].invert_yaxis()

plt.tight_layout()
plt.show()

# Create a binary variable 'conflict' based on whether there was a conflict in that year
# We assume that a conflict is indicated if the 'conflictyear' column is 1
df['conflict'] = df['conflictyear'].apply(lambda x: 1 if x == 1 else 0)

df

import plotly.graph_objs as go

# Find the top 10 actors with the most and least documented cases of sexualized violence
top_10_most = df_filtered.groupby('actor')['sexual_violence'].sum().nlargest(10).index
top_10_least = df_filtered.groupby('actor')['sexual_violence'].sum().nsmallest(10).index

# Filter the data for these actors
top_actors_most = df_filtered[df_filtered['actor'].isin(top_10_most)]
top_actors_least = df_filtered[df_filtered['actor'].isin(top_10_least)]

# Prepare data for plotting: Yearly summary for each actor
yearly_data_most = top_actors_most.groupby(['actor', 'year']).agg({
    'conflictyear': 'sum',
    'sexual_violence': 'sum'
}).reset_index()

yearly_data_least = top_actors_least.groupby(['actor', 'year']).agg({
    'conflictyear': 'sum',
    'sexual_violence': 'sum'
}).reset_index()

# Create traces for each actor (one trace for conflicts and one for sexual violence)
traces = []

# Adding traces for the top 10 actors with the most cases
for actor in top_10_most:
    actor_data = yearly_data_most[yearly_data_most['actor'] == actor]
    traces.append(go.Scatter(x=actor_data['year'], y=actor_data['conflictyear'],
                             mode='lines+markers', name=f'Conflict - {actor}', visible=True))
    traces.append(go.Scatter(x=actor_data['year'], y=actor_data['sexual_violence'],
                             mode='lines+markers', name=f'Violence - {actor}', visible=True))

# Adding traces for the top 10 actors with the least cases
for actor in top_10_least:
    actor_data = yearly_data_least[yearly_data_least['actor'] == actor]
    traces.append(go.Scatter(x=actor_data['year'], y=actor_data['conflictyear'],
                             mode='lines+markers', name=f'Conflict - {actor}', visible=True))
    traces.append(go.Scatter(x=actor_data['year'], y=actor_data['sexual_violence'],
                             mode='lines+markers', name=f'Violence - {actor}', visible=True))

# Create the figure
fig = go.Figure(data=traces)

# Update layout to add dropdowns for each actor
buttons = []
for i, actor in enumerate(top_10_most.tolist() + top_10_least.tolist()):
    button = dict(label=actor,
                  method="update",
                  args=[{"visible": [j == i * 2 or j == i * 2 + 1 for j in range(len(traces))]},
                        {"title": f"Conflict and Sexual Violence for {actor}"}])
    buttons.append(button)

fig.update_layout(
    updatemenus=[
        dict(active=0,
             buttons=buttons,
             direction="down",
             showactive=True,
        ),
    ],
    # Adjust width and height of the figure
    width=1600,
    height=1100,
    title="Top 10 Actors: Conflict Duration and Sexualized Violence",
    xaxis_title="Year",
    yaxis_title="Count"
)

fig.show()

# Creation of a binary variable 'conflict' based on whether there was a conflict in that year
# We assume that a conflict is indicated if the 'conflictyear' column is 1

df_filtered['conflict'] = df_filtered['conflictyear'].apply(lambda x: 1 if x == 1 else 0)

# Display the first few rows to verify the new column
df_filtered[['actor', 'conflict', 'sexual_violence']].head()

#carry out a statistical test to check whether there is a significant correlation between the presence of armed conflict and the occurrence of sexualised violence.
#We use a chi-square test for this.

import seaborn as sns
from scipy.stats import chi2_contingency


from scipy.stats import chi2_contingency

# Create a contingency table for the chi-square test
contingency_table = pd.crosstab(df_filtered['conflict'], df_filtered['sexual_violence'])

# Perform the chi-square test
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

# Output the results
chi2, p_value, dof, expected


# Visualization 1: Heatmap of the contingency table
plt.figure(figsize=(8, 6))
sns.heatmap(contingency_table, annot=True, fmt='d', cmap='coolwarm')
plt.title('Contingency Table: Conflict vs. Sexual Violence')
plt.xlabel('Sexual Violence')
plt.ylabel('Conflict')
plt.show()

# Step 1: Calculate the duration of the conflict for each actor
# We will assume that the 'conflictyear' column indicates whether a conflict was ongoing in a particular year (1 for conflict, 0 for no conflict)
# We will group by 'actor' and sum the 'conflictyear' column to get the duration of the conflict for each actor

df_filtered['conflict_duration'] = df_filtered.groupby('actor')['conflictyear'].transform('sum')

# Then we grouped by actor to get the total number of conflict years and total number of sexual violence reports
conflict_duration_vs_violence = df_filtered.groupby('actor').agg({
    'conflict_duration': 'max',  # maximum conflict duration for the actor
    'sexual_violence': 'sum'     # total number of sexual violence reports
}).reset_index()

# examine the first few rows of the result
conflict_duration_vs_violence.head()

# Create a scatter plot to visualize the relationship between conflict duration and sexual violence reports
plt.figure(figsize=(10, 6))
plt.scatter(conflict_duration_vs_violence['conflict_duration'], conflict_duration_vs_violence['sexual_violence'], color='blue', alpha=0.5)
plt.title('Conflict Duration vs. Sexualized Violence')
plt.xlabel('Conflict Duration')
plt.ylabel('Number of Sexual Violence Reports')
plt.grid(True)
plt.show()

# Carry out a chi-square test to check whether there is a significant correlation between the presence of armed conflict and the occurrence of sexualized violence
# Create a contingency table for the chi-square test
contingency_table = pd.crosstab(df['conflict'], df['sexual_violence'])

# Perform the chi-square test
chi2, p_value, dof, expected = chi2_contingency(contingency_table)

# Output the results of the chi-square test
print(f"Chi-square statistic: {chi2}, p-value: {p_value}, Degrees of freedom: {dof}")
print("Expected frequencies in contingency table:\n", expected)

columns_to_drop = ['child_prev', 'ai_prev', 'state_prev', 'hrw_prev', 'gwnoloc2']

# Drop the columns and create the updated dataframe
df = df.drop(columns=columns_to_drop)

# Display the first few rows of the updated dataframe to confirm the changes
df.head()

from scipy.stats import chi2_contingency

numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()
correlation_matrix = df[numerical_columns].corrwith(df['sexual_violence']).sort_values(ascending=False)

# Visualization of correlation
plt.figure(figsize=(10, 8))
sns.barplot(x=correlation_matrix.values, y=correlation_matrix.index, palette='coolwarm')
plt.title('Correlation with Sexual Violence')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.show()



# For categorial Variables Chi-Quadrat-Test
categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()

chi2_results = {}
for col in categorical_columns:
    contingency_table = pd.crosstab(df[col], df['sexual_violence'])
    chi2, p, dof, expected = chi2_contingency(contingency_table)
    chi2_results[col] = chi2


chi2_sorted = sorted(chi2_results.items(), key=lambda x: x[1], reverse=True)

plt.figure(figsize=(10, 8))
sns.barplot(x=[x[1] for x in chi2_sorted], y=[x[0] for x in chi2_sorted], palette='coolwarm')
plt.title('Chi-Square Statistic with Sexual Violence')
plt.xlabel('Chi-Square Value')
plt.ylabel('Categorical Features')
plt.show()

correlation_matrix = df[numerical_columns].corrwith(df['sexual_violence']).sort_values(ascending=False)

# Convert correlation values to a DataFrame for heatmap plotting
correlation_df = correlation_matrix.to_frame(name='correlation')

# Plotting the heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_df, annot=True, cmap='coolwarm', center=0)
plt.title('Heatmap of Correlation with Sexual Violence')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

#features:

#conflictyear: Represents the number of years a conflict has been active.
#conflict: The specific conflict being monitored.
#incomp: Type of incompatibility in the conflict (e.g. territorial dispute, governmental dispute).
#region: The geographical region in which the conflict takes place.
#actor: Actor involved in the conflict.
#gwnoloc: Location of the conflict (can contain information about the geography).
#actor_type: Type of actor (e.g. state, rebel group).
#interim and postc: Whether the conflict is in an interim or post-conflict phase.

# Feature-Selection
selected_features = ['conflictyear', 'conflict', 'incomp', 'region', 'actor', 'gwnoloc', 'actor_type', 'interm', 'postc']

# Prep Data
X = df[selected_features]
y = df['sexual_violence']

# Check for missing values
missing_values = X.isnull().sum()
print(f"Missing Values:\n{missing_values}")

#no imputation is necessary because there are no missing values
#RandomForestClassifier can only work with numerical data.
#We need to convert all categorical data to numeric data. This can be done by one-hot encoding, which means that a binary column is created for each categorical variable.

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
import matplotlib.pyplot as plt
import seaborn as sns

# One-Hot-Encoding for categorial variables
categorical_columns = ['conflict', 'region', 'actor', 'gwnoloc', 'actor_type', 'interm', 'postc']
for col in categorical_columns:
    X.loc[:, col] = X[col].astype(str)

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first'), categorical_columns)
    ],
    remainder='passthrough'

)


X_encoded = preprocessor.fit_transform(X)
print(f"Shape of transformed X: {X_encoded.shape}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix


X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)


rf = RandomForestClassifier(n_estimators=100, random_state=42)


rf.fit(X_train, y_train)


y_pred = rf.predict(X_test)


print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# top 20 important features
top_n = 20
top_indices = indices[:top_n]

# Plotten
feature_names = np.array(preprocessor.get_feature_names_out())

plt.figure(figsize=(12, 8))
plt.title(f"Top {top_n} Feature Importance")
plt.bar(range(top_n), importances[top_indices], align="center")
plt.xticks(range(top_n), feature_names[top_indices], rotation=90)
plt.xlim([-1, top_n])
plt.show()

# Split data in test and train
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)

# Over-Sampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# Train Random Forest Modells
rf = RandomForestClassifier(random_state=42)
rf.fit(X_resampled, y_resampled)

#Prediction on the test set
y_pred = rf.predict(X_test)

# how is the modell doing?
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusionsmatrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Train and Testset
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)

# Over-Sampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# Train Random Forest Modells
rf = RandomForestClassifier(random_state=42)
rf.fit(X_resampled, y_resampled)

# VPrediction Testset
y_pred = rf.predict(X_test)

# How does the modell work?
print("Classification Report:")
print(classification_report(y_test, y_pred))

# confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Saving Random Forest trained model"""

from joblib import dump

dump(rf, 'random_forest_model.joblib')

#Interpretation
#Precision:
#For class 0 (no occurrence of sexual violence), the precision is very high (0.98), which means that the model is correct in most cases when it predicts 0.
#For class 1 (occurrence of sexual violence), the precision is lower (0.37), indicating that the model has many false positives for this class.
#Recall:
#For class 1 is relatively high (0.82), indicating that the model recognises most actual occurrences of sexual violence.
#For class 0 is also high (0.85), which means that the model is also good at recognising non-occurrences.

#F1 score:
#The F1 score for class 0 is very high (0.91), which means that the model is good at correctly predicting non-occurrences of sexual violence.
#The F1 score for class 1 is lower (0.50), indicating an imbalance in model performance.

#Confusion matrix:
#The matrix shows that the model predicted 2,748 cases of 0 correctly and 279 cases of 1 correctly.
#However, there are 484 false negatives (the model says 0, but it was actually 1) and 63 false positives (the model says 1, but it was actually 0).

#--> we try XGboost

pip install xgboost

import xgboost as xgb
from xgboost import XGBClassifier

# One-Hot-Encoding
categorical_columns = ['conflict', 'region', 'actor', 'gwnoloc', 'actor_type', 'interm', 'postc']

# Convert all values into strings
for col in categorical_columns:
    X.loc[:, col] = X[col].astype(str)

# Use ColumnTransformer, for transforming the categorial columns
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first'), categorical_columns)
    ],
    remainder='passthrough'
)

# Transform features
X_encoded = preprocessor.fit_transform(X)

# split in train and testset
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42, stratify=y)

# Over-Sampling
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_train, y_train)

# XGBoost Model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_resampled, y_resampled)

# Prediction
y_pred = xgb_model.predict(X_test)

# How did it work?
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusionmatrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""Saving XGBoost trained model"""

dump(xgb_model, 'xgboost_model.joblib')

from sklearn.model_selection import GridSearchCV


param_grid = {
    'n_estimators': [100, 150],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'bootstrap': [True]
}


rf = RandomForestClassifier(random_state=42)


grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=2, scoring='f1')


grid_search.fit(X_resampled, y_resampled)


print("Best parameter according to GridSearch:", grid_search.best_params_)


best_rf = grid_search.best_estimator_


y_pred_best = best_rf.predict(X_test)


print("Classification Report:")
print(classification_report(y_test, y_pred_best))


conf_matrix_best = confusion_matrix(y_test, y_pred_best)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_best, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix with optimized Parameters')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#Class 1 (occurrence of sexual violence): Precision is 0.35, indicating that the model often makes false positive predictions for the occurrence of sexual violence.

from sklearn.model_selection import GridSearchCV
import xgboost as xgb

param_grid = {
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 1.0]
}


xgb_model = xgb.XGBClassifier(random_state=42)

# Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different
# iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.

# basic cross val

from sklearn.model_selection import cross_val_score

# perform cross validation using xgb model, focusing on precision

cv_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring='precision')
print(cv_scores)
print(cv_scores.mean())
print(cv_scores.std())

# The array of scores represents the accuracy scores for each iteration of cross-validation. Each value represents the accuracy achieved
# in a specific subset of the data. Each score indicates how well the model performed in each iteration.

# In K-fold cross-validation, the data set is divided into a number of K-folds and used to assess the model's ability as new data become available.
# K represents the number of groups into which the data sample is divided.  It involves splitting the dataset into k subsets or folds, where each fold
# is used as the validation set in turn while the remaining k-1 folds are used for training.

# define standard kfold

from sklearn.model_selection import KFold

# perform standard kfold using xgb model, focusing on precision

kf = KFold(n_splits=15, shuffle=True, random_state=42)
kf_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=kf, scoring='precision')
print(kf_scores)
print(kf_scores.mean())
print(kf_scores.std())

# Stratified K-Fold Cross-Validation addresses the limitations of standard K-Fold by ensuring that each fold has a balanced class distribution.

# stratified kfold

from sklearn.model_selection import StratifiedKFold

s_kf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)
s_kf_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=s_kf, scoring='precision')
print(s_kf_scores)
print(s_kf_scores.mean())
print(s_kf_scores.std())

# visualization of cross validation results

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='-', color='blue')
plt.title('Cross-Validation Scores (Precision)')
plt.xlabel('Fold')
plt.ylabel('Precision Score')
plt.grid(True)
plt.show()

# visualization of standard kfold

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(kf_scores) + 1), kf_scores, marker='o', linestyle='-', color='green')
plt.title('Standard K-Fold Cross-Validation Scores (Precision)')
plt.xlabel('Fold')
plt.ylabel('Precision Score')
plt.grid(True)
plt.show()

# visualization of stratified kfold

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(s_kf_scores) + 1), s_kf_scores, marker='o', linestyle='-', color='red')
plt.title('Stratified K-Fold Cross-Validation Scores (Precision)')
plt.xlabel('Fold')
plt.ylabel('Precision Score')
plt.grid(True)
plt.show()

# Boxplot of cross-validation scores

plt.figure(figsize=(8, 6))
plt.boxplot([cv_scores, kf_scores, s_kf_scores], labels=['Basic CV', 'KFold', 'Stratified KFold'])
plt.title('Cross-Validation Scores Comparison (Precision)')
plt.ylabel('Precision Score')
plt.show()

# perform cross validation using xgb model, focusing on recall

cv_scores_recall = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring='recall')
print(cv_scores_recall)
print(cv_scores_recall.mean())
print(cv_scores_recall.std())

kf = KFold(n_splits=15, shuffle=True, random_state=42)
kf_scores_recall = cross_val_score(xgb_model, X_resampled, y_resampled, cv=kf, scoring='recall')
print(kf_scores_recall)
print(kf_scores_recall.mean())
print(kf_scores_recall.std())

s_kf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)
s_kf_scores_recall = cross_val_score(xgb_model, X_resampled, y_resampled, cv=s_kf, scoring='recall')
print(s_kf_scores_recall)
print(s_kf_scores_recall.mean())
print(s_kf_scores_recall.std())

# perform cross validation on target classes (0 and 1 / absence and presence of sexual violence), using xgb model, focusing on recall and precision

from sklearn.metrics import make_scorer, recall_score
from sklearn.metrics import precision_score

recall_class_1_scorer = make_scorer(recall_score, pos_label=1)
recall_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring=recall_class_1_scorer)
print("Recall for class 1 across all folds:", recall_scores)
print("Mean Recall for class 1:", recall_scores.mean())

recall_class_0_scorer = make_scorer(recall_score, pos_label=0)
recall_scores_0 = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring=recall_class_0_scorer)
print("Recall for class 0 across all folds:", recall_scores_0)
print("Mean Recall for class 0:", recall_scores_0.mean())

precision_class_1_scorer = make_scorer(precision_score, pos_label=1)
precision_scores = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring=precision_class_1_scorer)
print("Precision for class 1 across all folds:", precision_scores)
print("Mean Precision for class 1:", precision_scores.mean())

precision_class_0_scorer = make_scorer(precision_score, pos_label=0)
precision_scores_0 = cross_val_score(xgb_model, X_resampled, y_resampled, cv=15, scoring=precision_class_0_scorer)
print("Precision for class 0 across all folds:", precision_scores_0)
print("Mean Precision for class 0:", precision_scores_0.mean())



# perform cross validation on target classes (0 and 1 / absence and presence of sexual violence), using RandomForest model, focusing on recall and precision

recall_class_1_scorer = make_scorer(recall_score, pos_label=1)
recall_scores = cross_val_score(rf, X_resampled, y_resampled, cv=15, scoring=recall_class_1_scorer)
print("Recall for class 1 across all folds:", recall_scores)
print("Mean Recall for class 1:", recall_scores.mean())

recall_class_0_scorer = make_scorer(recall_score, pos_label=0)
recall_scores_0 = cross_val_score(rf, X_resampled, y_resampled, cv=15, scoring=recall_class_0_scorer)
print("Recall for class 0 across all folds:", recall_scores_0)
print("Mean Recall for class 0:", recall_scores_0.mean())

precision_class_1_scorer = make_scorer(precision_score, pos_label=1)
precision_scores = cross_val_score(rf, X_resampled, y_resampled, cv=15, scoring=precision_class_1_scorer)
print("Precision for class 1 across all folds:", precision_scores)
print("Mean Precision for class 1:", precision_scores.mean())

precision_class_0_scorer = make_scorer(precision_score, pos_label=0)
precision_scores_0 = cross_val_score(rf, X_resampled, y_resampled, cv=15, scoring=precision_class_0_scorer)
print("Precision for class 0 across all folds:", precision_scores_0)
print("Mean Precision for class 0:", precision_scores_0.mean())

!pip install shap
import shap

# creating an explainer for our model
# we only need to pass our fitted model to tree explainer.
# We select TreeExplainer here since XGBoost is a tree-based model.

explainer = shap.TreeExplainer(xgb_model)

# finding out the shap values using the explainer
shap_values = explainer.shap_values(X_train)

# Expected value = the value that would be predicted if we didn’t know any features of the current output”
print('Expected Value:', explainer.expected_value)

# Each row belongs to a single prediction made by the model.
# Each column represents a feature used in the model.
# Each SHAP value represents how much this feature contributes to the output of this row’s prediction.
print(shap_values)
print(shap_values.shape)

# Positive SHAP value means positive impact on prediction, leading the model to predict 1
# (presence of sexual violence).
# Negative SHAP value means negative impact, leading the model to predict 0
# (no sexual violence).

feature_names = feature_names = np.array(preprocessor.get_feature_names_out())

# Convert the csr_matrix to a DataFrame
X_train_df = pd.DataFrame.sparse.from_spmatrix(X_train, columns=feature_names)

# Feature importance

shap.summary_plot(shap_values, X_train_df, plot_type='bar')

# Directionality impact

# Red color means higher value of a feature. Blue means lower value of a feature.
# For example, Higher value of “actor_type_2.0” leads to lower chance to sexual violence.
# Lower value of “actor_type_2.0” leads to higher chance of sexual violence.

shap.summary_plot(shap_values, X_train_df)

#SHAP Summary Plot (Beeswarm Plot):
#Interpretation: The SHAP summary plot (first image) provides an overall view of the distribution of SHAP values for each feature across all samples. Each dot represents a SHAP value for a particular feature in a particular sample.
#Color: The color represents the feature value (e.g., red for high values, blue for low values).
#Position: The position of the dots on the x-axis shows the SHAP value, which indicates how much that feature influenced the prediction (positive SHAP values push towards sexual violence, negative values push towards no sexual violence).
#Key insights:
#cat__actor_type_2.0 (likely representing a specific actor type, such as state actors) and cat__actor_type_6.0 (possibly pro-government militias or other actor types) are the most influential features. When these actor types are present, the model is more likely to predict no sexual violence (since most of their SHAP values are negative).
#cat__actor_type_3.0 (rebel groups) has a significant positive SHAP value, indicating that the involvement of rebel groups strongly increases the likelihood of sexual violence being predicted.
#cat__postc_1 (post-conflict status) generally has a negative SHAP value, meaning that the occurrence of a conflict in the post-conflict period decreases the likelihood of sexual violence.


# Explaining individual predictions:
# Create an Explanation object
explanation = shap.Explanation(values=shap_values[0],
                                base_values=explainer.expected_value,
                                data=X_train_df.iloc[0],
                                feature_names=X_train_df.columns)

# Plot the waterfall plot using the Explanation object
shap.plots.waterfall(explanation)

# The input variables are ranked from top to bottom by how much impact they have on the model's prediction
# We can see that "actor_type_4.0" has strong impact on predicting absence of sexual violence.
# However, "actor_type_3.0" feature 'push' towards predicting presence of sexual violence.

# Force plot

shap.initjs()
shap.force_plot(
    explainer.expected_value,
    shap_values[0, :],
    X_train_df.iloc[0, :])

# SHAP values that 'push' the model towards a higher value (sexual violence) appear on the left in red
# SHAP values that 'push' the model towards a lower value ( no sexual violence) appear on the right in blue
# Variables with larger SHAP values (larger arrows) have more impact